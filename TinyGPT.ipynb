{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de54788f",
   "metadata": {},
   "source": [
    "# TinyGPT Model for NLP-II course\n",
    "\n",
    "### Juan Ignacio García (a2008)\n",
    "\n",
    "This file contains all dependencies necessary to train and test a small-volume GPT model based on Shakespeare's plays. The model aims to use the *Mixture of Experts* technique to train and use *Greedy Decoding*, *Temperature Sampling* & *Top k / Top p Sampling* for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8036f",
   "metadata": {},
   "source": [
    "## Library import\n",
    "\n",
    "This section is used to centralize every library used for developing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16d2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
    "import math\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ea6e3",
   "metadata": {},
   "source": [
    "## Downloading the database\n",
    "\n",
    "We use the **[tiny_shakespeare](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)** dataset, by Andrej Karpathy; a dataset featuring 40000 lines of Shakespeare from a variety of Shakespeare's plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1826c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = httpx.get(url)\n",
    "text = response.text\n",
    "print(text[:100])  # Print the first 1000 characters to verify download\n",
    "\n",
    "# Save the text to a file\n",
    "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d648d8c",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "This dataset is composed by a small corpus (~1MB) with a small character range (~65 characters) in an old-fashioned english wording style. Based on this analysis, the optimal tokenizer to use should be a **Character-Level tokenizer** allowed to infer automatically gramatic & ortographic rules even for words outside the vocabulary. This is specially useful in this case as there are multiple wording variations inside Shakespeare's plays.\n",
    "\n",
    "However, I propose using a **Byte-Pair tokenizer** that creates subwords specifically trained using this dataset, allowing this way to mantain some flexibility to old wordings while conforming larger strings to obtain better wording representation as an output. This will also provide some comarison base with the material provided by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f6f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "try:\n",
    "    # Try to load an existing tokenizer\n",
    "    tokenizer = Tokenizer.from_file(\"tokenizer_bpe_shakespeare/tokenizer.json\")\n",
    "    print(\"Loaded existing tokenizer.\")\n",
    "except:\n",
    "    # Train it over corpus\n",
    "    tokenizer.train(\n",
    "        files=[\"shakespeare.txt\"],\n",
    "        vocab_size=8000,\n",
    "        min_frequency=2,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "    )\n",
    "    print(\"Trained new tokenizer.\")\n",
    "\n",
    "    # Save the tokenizer model files\n",
    "    tokenizer.save_model(\"tokenizer_bpe_shakespeare\")\n",
    "    tokenizer.save(\"tokenizer_bpe_shakespeare/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3f029",
   "metadata": {},
   "source": [
    "Here is a sample of the trained tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1c78673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'Ġmore', 'Ġtalking', 'Ġon', \"'t\", ';', 'Ġlet', 'Ġit', 'Ġbe', 'Ġdone', ':', 'Ġaway', ',', 'Ġaway', '!']\n",
      "[694, 490, 6854, 374, 672, 31, 543, 344, 310, 846, 30, 954, 16, 954, 5]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"No more talking on't; let it be done: away, away!\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1189f",
   "metadata": {},
   "source": [
    "After training the tokenizer, we must set up the tensorial representation of the dataset and conform a dataloader structure to easily feed up the model. In order to do this, we create the `ShakespeareDataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9b87a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049fe22",
   "metadata": {},
   "source": [
    "To represent the original data in this tokenized form, we first divide the text in training & testing datasets and then create individual instances of the `ShakespeareDataset` tokenized version for each one of them. After that, I set up a `Dataloader` object provisioned with the right dependencies to easily feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0efec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIO = 0.8\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "data = tokenizer.encode(text).ids\n",
    "n = int(TRAIN_TEST_RATIO * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Create datset instances\n",
    "block_size = 128\n",
    "train_ds = ShakespeareDataset(train_data, block_size)\n",
    "val_ds = ShakespeareDataset(val_data, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0744027",
   "metadata": {},
   "source": [
    "## Mixture of Experts & Transformer Layer\n",
    "\n",
    "This first layer structure provides a series of \"experts\" as smaller specialized neural networks dedicated to interpretate particular tokens to better understand the context of a phrase. The model redirects a token to a selection of experts using a router. This method is useful to improve the understanding and representation of the context from a certain text while making efficient the inference cost for training the model.\n",
    "\n",
    "To implement this, we create a `MoE` class that manages instances of a second `FeedForward` class serving as the expert. The `FeedForward` class is composed of a small neural network using a GELU activation function as a way to introduce non-linearities. The `MoE`class also features another linear layer as a gate for the tokens that enter and are redirected to the selected experts. After *gating* or processing the probability of every expert of handling a certain token, we select the top *k* experts of the list to process the token and weight the results at the output.\n",
    "\n",
    "The data feeded to the model must be a structure composed of a tensor representing the encoded input, with parameters `B`, `T` & `C` defining the batch size, sequence size and embedding dimension used on a certain data piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34183a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple token-wise MoE.\n",
    "    - experts: list of FFNs\n",
    "    - gating: linear over features -> scores over experts\n",
    "    - top_k routing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim, n_experts=4, capacity_factor=1.0, top_k=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "        # Experts: each is a FeedForward (can be any module)\n",
    "        self.experts = nn.ModuleList([FeedForward(dim, hidden_dim) for _ in range(n_experts)])\n",
    "\n",
    "        # Gating network: project token representation to logits over experts\n",
    "        self.gate = nn.Linear(dim, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        flat = x.view(B*T, C)  # (BT, C)\n",
    "\n",
    "        logits = self.gate(flat)  # (BT, n_experts)\n",
    "\n",
    "        # Softmax probabilities over experts\n",
    "        probs = F.softmax(logits, dim=-1)  # (BT, n_experts)\n",
    "\n",
    "        # top-k indices and values\n",
    "        topk_vals, topk_idx = torch.topk(probs, k=self.top_k, dim=-1)  # (BT, top_k)\n",
    "\n",
    "        # For top-1 routing, choose expert per token\n",
    "        if self.top_k == 1:\n",
    "            expert_choice = topk_idx.squeeze(-1)  # (BT,)\n",
    "            outputs = torch.zeros_like(flat)\n",
    "\n",
    "            # Efficient per-expert processing: gather positions for each expert\n",
    "            for e in range(self.n_experts):\n",
    "                mask = (expert_choice == e)\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                selected = flat[mask]  # (n_e, C)\n",
    "                out_e = self.experts[e](selected)  # (n_e, C)\n",
    "                outputs[mask] = out_e\n",
    "\n",
    "            outputs = outputs.view(B, T, C)\n",
    "            return outputs, probs.view(B, T, self.n_experts)\n",
    "        else:\n",
    "            # top-k mixture: weighted sum of top-k experts' outputs\n",
    "            out = torch.zeros_like(flat)\n",
    "\n",
    "            # Compute all expert outputs on every token and multiply\n",
    "            all_outs = []\n",
    "            for e in range(self.n_experts):\n",
    "                out_e = self.experts[e](flat)  # (BT, C)\n",
    "                all_outs.append(out_e)\n",
    "            all_outs = torch.stack(all_outs, dim=1)  # (BT, n_experts, C)\n",
    "\n",
    "            # Multiply by probs and sum over experts (but only top-k could be masked)\n",
    "            probs_mask = torch.zeros_like(all_outs[:, :, 0:1])  # (BT, n_experts, 1)\n",
    "            probs_mask[:, :, 0] = 0.0  # initializer\n",
    "\n",
    "            # Create mask for top-k\n",
    "            mask_topk = torch.zeros_like(probs)  # (BT, n_experts)\n",
    "            for j in range(self.top_k):\n",
    "                mask_topk.scatter_(1, topk_idx[:, j:j+1], 1.0)\n",
    "            mask_topk = mask_topk.unsqueeze(-1)  # (BT, n_experts, 1)\n",
    "            weighted = all_outs * (probs.unsqueeze(-1) * mask_topk)\n",
    "            out = weighted.sum(dim=1)  # (BT, C)\n",
    "            out = out.view(B, T, C)\n",
    "            return out, probs.view(B, T, self.n_experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29465fbe",
   "metadata": {},
   "source": [
    "The other useful layer for our model is a *Transformer* layer that will combine the previous MoE layer with a multilayer head attention mechanism to better understand how a word relates to another and infer the imported context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "422e6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockMoE(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_hidden_dim, n_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # MoE layer\n",
    "        self.moe = MoE(dim, mlp_hidden_dim, n_experts=n_experts, top_k=top_k)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=None)  # (B, T, C)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        moe_out, gate_probs = self.moe(x)  # (B, T, C), (B, T, n_experts)\n",
    "        x = x + self.dropout(moe_out)\n",
    "        x = self.ln2(x)\n",
    "        return x, gate_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440b8a9",
   "metadata": {},
   "source": [
    "We finally combine these layers into a structure that also trains an embedding of the tokenized tensors combined with a positional encoder to attend not just at the tokens present in a phrase but also their arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "697d4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPTMoE(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layers=4, dim=128, n_heads=4,\n",
    "                 mlp_hidden_dim=512, n_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.dim = dim\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, dim)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlockMoE(dim, n_heads, mlp_hidden_dim, n_experts=n_experts, top_k=top_k)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, T)\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        tok = self.token_emb(idx)  # (B, T, C)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))[None, :, :]\n",
    "\n",
    "        x = self.drop(tok + pos)\n",
    "\n",
    "        gate_probs_all = []\n",
    "        for blk in self.blocks:\n",
    "            x, gate_probs = blk(x)\n",
    "            gate_probs_all.append(gate_probs)  # list of (B, T, n_experts)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab)\n",
    "        # stack gate probs along layers -> (n_layers, B, T, n_experts)\n",
    "        gate_probs_all = torch.stack(gate_probs_all, dim=0)\n",
    "        return logits, gate_probs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd6fcd",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To simplify the training process, I provide a training helper using an `AdamW` optimizer, a schedular with decaying learning rate and defining an evaluation function comparing the cross-entropy loss between the predicted and correct answers on the validation dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9fa59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(dataloader, desc=\"Evaluando\", leave=False):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits, _ = model(xb)\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), yb.view(B*T), reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "            n_tokens += B*T\n",
    "    model.train()\n",
    "    return total_loss / n_tokens\n",
    "\n",
    "def train(tokenizer, train_loader, val_loader, device, block_size=128, n_layers=4, dim=128, n_heads=4, mlp_hidden_dim=512,\n",
    "          n_experts=4, top_k=1, lr=3e-4, epochs=20, ckpt_path=\"tinygpt_moe.pth\"):\n",
    "\n",
    "    vocab_size = len(tokenizer.get_vocab())\n",
    "    model = TinyGPTMoE(vocab_size=vocab_size, block_size=block_size,\n",
    "                       n_layers=n_layers, dim=dim, n_heads=n_heads,\n",
    "                       mlp_hidden_dim=mlp_hidden_dim, n_experts=n_experts, top_k=top_k)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-1)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_tokens = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "        for i, (xb, yb) in enumerate(progress_bar, start=1):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits, gate_probs = model(xb)\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), yb.view(B*T))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * B*T\n",
    "            n_tokens += B*T\n",
    "            \n",
    "        scheduler.step()\n",
    "        val_loss = evaluate(model, val_loader, device)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        print(f\"==> Epoch {epoch} validation loss {val_loss:.4f} ppl {val_ppl:.2f}\")\n",
    "\n",
    "        # save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state': model.state_dict(),\n",
    "                'tokenizer': tokenizer.__dict__\n",
    "            }, ckpt_path)\n",
    "            print(\"Modelo guardado en\", ckpt_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe9e4b",
   "metadata": {},
   "source": [
    "We set the needed parameters and start the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb713d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37105336d43b483ba9fe1aa349058a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68594cf15ef4dfaa836465f6356344f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 0 validation loss 0.8396 ppl 2.32\n",
      "Modelo guardado en tinygpt_moe.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740b77313a8a45c8a0c00eea0f93ddae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529bd289bb1140fd8cd79042baf4697d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 validation loss 0.4756 ppl 1.61\n",
      "Modelo guardado en tinygpt_moe.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d718ee9aca14f5b98660cbd357745d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e849a4e5951d416fb6b44b180427b4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 2 validation loss 0.4765 ppl 1.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8550d389a6a7405f9ff7402849f717e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c187e3a5021d475089c61ae1006d3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 3 validation loss 0.4903 ppl 1.63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d963980954d4e6fb135ea9865e3b50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d160c79c6434f9c7a9ab01295fcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 4 validation loss 0.4948 ppl 1.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce18f11abbd413997b14af3d94d1587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba15a165b364ac8b7a9d36a880293c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 5 validation loss 0.5030 ppl 1.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea78147454f46899314e4b8809b9089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8822ecbb6caa4c1a9463bd2adba595b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 6 validation loss 0.5137 ppl 1.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05edc21ea4694e1887e80674bdac1812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dd61c9da5c488cb92b420a350321af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 7 validation loss 0.5207 ppl 1.68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e857e6eb2e440fc89197f00c6999029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5907bfbeac48423f8bcf1501a22205c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 8 validation loss 0.5302 ppl 1.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015ebfc494bc4509b237ed12c681f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f0f157f5314ee685ad1c0e8c64fed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 9 validation loss 0.5356 ppl 1.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f7a7372beb4704bc6ce1891005e707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e65519f64104f529fc2c500250eb5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 10 validation loss 0.5497 ppl 1.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a0474e69e9414aaaac9d1080ef166e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d15c8ec45274836b091002c21c14a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 11 validation loss 0.5556 ppl 1.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81052b71e2fb4203bc646cd076a2052b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc537c2dedc48d49370790cfb1ba590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 12 validation loss 0.5611 ppl 1.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9aa61142f445deb7ad03282cd573f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f65d2eea42476da927a2ccd55b7bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 13 validation loss 0.5640 ppl 1.76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d387b18eff78440b8b820cd1e986813b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54866082662a4ddb857fbfbe534d0a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 14 validation loss 0.5720 ppl 1.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7768f6f7bc0d4035b23612e08840ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a741d421d64842bbf201b0c44632e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 15 validation loss 0.5786 ppl 1.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf468ce6770045dd9db51c558b6d9036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ccaa8b596848a19108677daef65311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 16 validation loss 0.5736 ppl 1.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8962dc46062b4d489d1e1d6831eea5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e0722e31f546ec81b91cf946ae19e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 17 validation loss 0.5747 ppl 1.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f51f5fb6aa4610b791da7347c969e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7abc9763a548d2a7632fdc9829618e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 18 validation loss 0.5735 ppl 1.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99fcc4a64384ad083ed803e3f562dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cf5bfe12f343ac860a301b53fc6990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluando:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 19 validation loss 0.5782 ppl 1.78\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "block_size = 128\n",
    "n_layers = 4\n",
    "dim = 128\n",
    "n_heads = 4\n",
    "mlp_hidden_dim = 512\n",
    "n_experts = 4\n",
    "top_k = 1\n",
    "lr = 3e-4\n",
    "epochs = 20\n",
    "ckpt_path = \"tinygpt_moe.pth\"\n",
    "\n",
    "# Start training\n",
    "model = train(tokenizer, train_loader, val_loader, DEVICE,\n",
    "      block_size=block_size,\n",
    "      n_layers=n_layers,\n",
    "      dim=dim,\n",
    "      n_heads=n_heads,\n",
    "      mlp_hidden_dim=mlp_hidden_dim,\n",
    "      n_experts=n_experts,\n",
    "      top_k=top_k,\n",
    "      lr=lr,\n",
    "      epochs=epochs,\n",
    "      ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d143c",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "After training the model we are able to produce inferences based on a certain seed. The model will provide a list of tokens oredered by descending probability to be chosen and we define three different strategies to ensamble the tokens to produce a coherent response. The algorithms chosen for this operation are:\n",
    "\n",
    "* Greedy approach: This is the most straightforward algorithm and consist on choosing the token with the highest probability.\n",
    "\n",
    "* Top-k approach: We select the *k* most probable tokens and then choose one of them at random, weighted by their probabilities.\n",
    "\n",
    "* Top-p approach: We select groups of tokens called *nucleus* where tokens are included until the cummulative probability goes over a threshold *p*. Then we choose one of them at random, weighted by their probabilities.\n",
    "\n",
    "Additionally, we implement a *Temperature Sampling* mechanism that narrows or expands the ability of the infered phrase to allow a wide variety of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b28956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_greedy(model, tokenizer, seed_text, max_new_tokens=50, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    idx = torch.tensor([tokenizer.encode(seed_text).ids], dtype=torch.long, device=device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        if idx.shape[1] > model.block_size:\n",
    "            idx = idx[:, -model.block_size:]\n",
    "        logits, _ = model(idx)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "    out = model.token_emb.weight.device\n",
    "    return tokenizer.decode(idx[0].tolist())\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_p_filtering(logits, top_p=0.9, filter_value=-float(\"Inf\")):\n",
    "    \"\"\"Filtra logits según probabilidad acumulada top-p (nucleus).\"\"\"\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # Determina los índices a filtrar\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Siempre dejamos al menos el primer token\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "    # Reconstruimos el tensor original\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "    return logits\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sample(model, tokenizer, seed_text, max_new_tokens, device,\n",
    "                    temperature=1.0, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # tokenizar entrada\n",
    "    encoded = tokenizer.encode(seed_text).ids\n",
    "    idx = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # recortar si se pasa del tamaño de contexto\n",
    "        if idx.shape[1] > model.block_size:\n",
    "            idx_cond = idx[:, -model.block_size:]\n",
    "        else:\n",
    "            idx_cond = idx\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        # --- top-k filtering ---\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            min_values = values[:, -1].unsqueeze(1)\n",
    "            logits = torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "        # --- top-p (nucleus) filtering ---\n",
    "        if top_p is not None and 0 < top_p < 1.0:\n",
    "            logits = top_p_filtering(logits, top_p)\n",
    "\n",
    "        # --- softmax y muestreo ---\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # normalizamos por si hay NaN o todos inf\n",
    "        probs = torch.nan_to_num(probs, nan=0.0)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        if torch.all(probs == 0):\n",
    "            # fallback a greedy si las probabilidades son inválidas\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3efad",
   "metadata": {},
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be5b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejemplos de generación ---\n",
      "Greedy:\n",
      " To be, or, except necessity except necessity necessity except necessity beats earl except except necessity earl succor earl except except necessity necessity except succor necessity or earl or except monster succor succor succor succor succor succor succor earl our except necessity necessity earl except necessity necessity necessity necessity necessity except necessity necessity\n",
      "\n",
      "Sample (temp=1.0, top_k=40):\n",
      " To be, or, perhaps corruptionbt runs knave without runs pleasant descend corruption knave landed runs daresged withoutselves cameged corruption I contineem b yoke denial shaical factged came beauty red heinous awret attendhalleem speworder Richard wreck eyes b thin eyes lute\n",
      "\n",
      "Sample (temp=0.8, top_p=0.9):\n",
      " To be, orienttwixt, when plainly, making wilt graft graft\n",
      "Commit hast wouldst swearuted making noblyienttwixt point obey to know doct know entreats know know know know think rests fright dismiss know corrupt know keeps know redeem graft gravesient graft rests acquaint graft Menenius\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# final sample\n",
    "print(\"\\n--- Ejemplos de generación ---\")\n",
    "seed = \"To be, or\"\n",
    "print(\"Greedy:\\n\", generate_greedy(model, tokenizer, seed, max_new_tokens=50, device=DEVICE))\n",
    "print(\"\\nSample (temp=1.0, top_k=40):\\n\", generate_sample(model, tokenizer, seed, max_new_tokens=50,\n",
    "                                                        device=DEVICE, temperature=1.4, top_k=30))\n",
    "print(\"\\nSample (temp=0.8, top_p=0.9):\\n\", generate_sample(model, tokenizer, seed, max_new_tokens=50,\n",
    "                                                            device=DEVICE, temperature=1.4, top_p=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a19ef7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejemplos de generación ---\n",
      "Greedy:\n",
      " ROMEO:\n",
      "ROMEO next pan: pan: next p p next pats our pan id kings: di pan feather next p next pan next panan pats panats liking di p next apats p next patsards:\n",
      "\n",
      "Sample (temp=1.0, top_k=40):\n",
      " ROMEO:\n",
      "ROMEO: likingump liking speak French relvanceigence Juliet liking Juliet Julietthy born liking lovers vialalksakes digump witnessesROMEO bailingers liking Juliet rel vial liking Juliet thrusther acc digiz flower colour colour liking Juliet nobly loversnat Juliet ch French Juliet\n",
      "\n",
      "Sample (temp=0.8, top_p=0.9):\n",
      " ROMEO:\n",
      "ROMEO Verona side blows drop flight lasting side staysind loinsoses coldetAyWiltaster side is Verona dep side sideper wat alarsc removed browthe sideaster feet blackaster mor Hastings Verona-of revengedlencess waget fashion hour late correction thirty\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# final sample\n",
    "print(\"\\n--- Ejemplos de generación ---\")\n",
    "seed = \"ROMEO:\\n\"\n",
    "print(\"Greedy:\\n\", generate_greedy(model, tokenizer, seed, max_new_tokens=50, device=DEVICE))\n",
    "print(\"\\nSample (temp=1.0, top_k=40):\\n\", generate_sample(model, tokenizer, seed, max_new_tokens=50,\n",
    "                                                        device=DEVICE, temperature=1.6, top_k=30))\n",
    "print(\"\\nSample (temp=0.8, top_p=0.9):\\n\", generate_sample(model, tokenizer, seed, max_new_tokens=50,\n",
    "                                                            device=DEVICE, temperature=1.6, top_p=0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLO_Old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

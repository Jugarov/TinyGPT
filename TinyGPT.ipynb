{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de54788f",
   "metadata": {},
   "source": [
    "# TinyGPT Model for NLP-II course\n",
    "\n",
    "### Juan Ignacio García (a2008)\n",
    "\n",
    "This file contains all dependencies necessary to train and test a small-volume GPT model based on Shakespeare's plays. The model aims to use the *Mixture of Experts* technique to train and use *Greedy Decoding*, *Temperature Sampling* & *Top k / Top p Sampling* for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8036f",
   "metadata": {},
   "source": [
    "## Library import\n",
    "\n",
    "This section is used to centralize every library used for developing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c16d2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JuanI\\miniconda3\\envs\\TinyGPT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
    "import math\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ea6e3",
   "metadata": {},
   "source": [
    "## Downloading the database\n",
    "\n",
    "We use the **[tiny_shakespeare](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)** dataset, by Andrej Karpathy; a dataset featuring 40000 lines of Shakespeare from a variety of Shakespeare's plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1826c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = httpx.get(url)\n",
    "text = response.text\n",
    "print(text[:100])  # Print the first 1000 characters to verify download\n",
    "\n",
    "# Save the text to a file\n",
    "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d648d8c",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "This dataset is composed by a small corpus (~1MB) with a small character range (~65 characters) in an old-fashioned english wording style. Based on this analysis, the optimal tokenizer to use should be a **Character-Level tokenizer** allowed to infer automatically gramatic & ortographic rules even for words outside the vocabulary. This is specially useful in this case as there are multiple wording variations inside Shakespeare's plays.\n",
    "\n",
    "However, I propose using a **Byte-Pair tokenizer** that creates subwords specifically trained using this dataset, allowing this way to mantain some flexibility to old wordings while conforming larger strings to obtain better wording representation as an output. This will also provide some comarison base with the material provided by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23f6f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "try:\n",
    "    # Try to load an existing tokenizer\n",
    "    tokenizer = Tokenizer.from_file(\"tokenizer_bpe_shakespeare/tokenizer.json\")\n",
    "    print(\"Loaded existing tokenizer.\")\n",
    "except:\n",
    "    # Train it over corpus\n",
    "    tokenizer.train(\n",
    "        files=[\"shakespeare.txt\"],\n",
    "        vocab_size=8000,\n",
    "        min_frequency=2,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "    )\n",
    "    print(\"Trained new tokenizer.\")\n",
    "\n",
    "    # Save the tokenizer model files\n",
    "    tokenizer.save_model(\"tokenizer_bpe_shakespeare\")\n",
    "    tokenizer.save(\"tokenizer_bpe_shakespeare/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3f029",
   "metadata": {},
   "source": [
    "Here is a sample of the trained tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1c78673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'Ġmore', 'Ġtalking', 'Ġon', \"'t\", ';', 'Ġlet', 'Ġit', 'Ġbe', 'Ġdone', ':', 'Ġaway', ',', 'Ġaway', '!']\n",
      "[694, 490, 6854, 374, 672, 31, 543, 344, 310, 846, 30, 954, 16, 954, 5]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"No more talking on't; let it be done: away, away!\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1189f",
   "metadata": {},
   "source": [
    "After training the tokenizer, we must set up the tensorial representation of the dataset and conform a dataloader structure to easily feed up the model. In order to do this, we create the `ShakespeareDataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9b87a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049fe22",
   "metadata": {},
   "source": [
    "To represent the original data in this tokenized form, we first divide the text in training & testing datasets and then create individual instances of the `ShakespeareDataset` tokenized version for each one of them. After that, I set up a `Dataloader` object provisioned with the right dependencies to easily feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e0efec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIO = 0.8\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "data = tokenizer.encode(text).ids\n",
    "n = int(TRAIN_TEST_RATIO * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Create datset instances\n",
    "block_size = 128\n",
    "train_ds = ShakespeareDataset(train_data, block_size)\n",
    "val_ds = ShakespeareDataset(val_data, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0744027",
   "metadata": {},
   "source": [
    "## Mixture of Experts & Transformer Layer\n",
    "\n",
    "This first layer structure provides a series of \"experts\" as smaller specialized neural networks dedicated to interpretate particular tokens to better understand the context of a phrase. The model redirects a token to a selection of experts using a router. This method is useful to improve the understanding and representation of the context from a certain text while making efficient the inference cost for training the model.\n",
    "\n",
    "To implement this, we create a `MoE` class that manages instances of a second `FeedForward` class serving as the expert. The `FeedForward` class is composed of a small neural network using a GELU activation function as a way to introduce non-linearities. The `MoE`class also features another linear layer as a gate for the tokens that enter and are redirected to the selected experts. After *gating* or processing the probability of every expert of handling a certain token, we select the top *k* experts of the list to process the token and weight the results at the output.\n",
    "\n",
    "The data feeded to the model must be a structure composed of a tensor representing the encoded input, with parameters `B`, `T` & `C` defining the batch size, sequence size and embedding dimension used on a certain data piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a34183a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple token-wise MoE.\n",
    "    - experts: list of FFNs\n",
    "    - gating: linear over features -> scores over experts\n",
    "    - top_k routing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim, n_experts=4, capacity_factor=1.0, top_k=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "        # Experts: each is a FeedForward (can be any module)\n",
    "        self.experts = nn.ModuleList([FeedForward(dim, hidden_dim) for _ in range(n_experts)])\n",
    "\n",
    "        # Gating network: project token representation to logits over experts\n",
    "        self.gate = nn.Linear(dim, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        flat = x.view(B*T, C)  # (BT, C)\n",
    "\n",
    "        logits = self.gate(flat)  # (BT, n_experts)\n",
    "\n",
    "        # Softmax probabilities over experts\n",
    "        probs = F.softmax(logits, dim=-1)  # (BT, n_experts)\n",
    "\n",
    "        # top-k indices and values\n",
    "        topk_vals, topk_idx = torch.topk(probs, k=self.top_k, dim=-1)  # (BT, top_k)\n",
    "\n",
    "        # For top-1 routing, choose expert per token\n",
    "        if self.top_k == 1:\n",
    "            expert_choice = topk_idx.squeeze(-1)  # (BT,)\n",
    "            outputs = torch.zeros_like(flat)\n",
    "\n",
    "            # Efficient per-expert processing: gather positions for each expert\n",
    "            for e in range(self.n_experts):\n",
    "                mask = (expert_choice == e)\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                selected = flat[mask]  # (n_e, C)\n",
    "                out_e = self.experts[e](selected)  # (n_e, C)\n",
    "                outputs[mask] = out_e\n",
    "\n",
    "            outputs = outputs.view(B, T, C)\n",
    "            return outputs, probs.view(B, T, self.n_experts)\n",
    "        else:\n",
    "            # top-k mixture: weighted sum of top-k experts' outputs\n",
    "            out = torch.zeros_like(flat)\n",
    "\n",
    "            # Compute all expert outputs on every token and multiply\n",
    "            all_outs = []\n",
    "            for e in range(self.n_experts):\n",
    "                out_e = self.experts[e](flat)  # (BT, C)\n",
    "                all_outs.append(out_e)\n",
    "            all_outs = torch.stack(all_outs, dim=1)  # (BT, n_experts, C)\n",
    "\n",
    "            # Multiply by probs and sum over experts (but only top-k could be masked)\n",
    "            probs_mask = torch.zeros_like(all_outs[:, :, 0:1])  # (BT, n_experts, 1)\n",
    "            probs_mask[:, :, 0] = 0.0  # initializer\n",
    "\n",
    "            # Create mask for top-k\n",
    "            mask_topk = torch.zeros_like(probs)  # (BT, n_experts)\n",
    "            for j in range(self.top_k):\n",
    "                mask_topk.scatter_(1, topk_idx[:, j:j+1], 1.0)\n",
    "            mask_topk = mask_topk.unsqueeze(-1)  # (BT, n_experts, 1)\n",
    "            weighted = all_outs * (probs.unsqueeze(-1) * mask_topk)\n",
    "            out = weighted.sum(dim=1)  # (BT, C)\n",
    "            out = out.view(B, T, C)\n",
    "            return out, probs.view(B, T, self.n_experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29465fbe",
   "metadata": {},
   "source": [
    "The other useful layer for our model is a *Transformer* layer that will combine the previous MoE layer with a multilayer head attention mechanism to better understand how a word relates to another and infer the imported context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "422e6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockMoE(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_hidden_dim, n_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # MoE layer\n",
    "        self.moe = MoE(dim, mlp_hidden_dim, n_experts=n_experts, top_k=top_k)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=None)  # (B, T, C)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        moe_out, gate_probs = self.moe(x)  # (B, T, C), (B, T, n_experts)\n",
    "        x = x + self.dropout(moe_out)\n",
    "        x = self.ln2(x)\n",
    "        return x, gate_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440b8a9",
   "metadata": {},
   "source": [
    "We finally combine these layers into a structure that also trains an embedding of the tokenized tensors combined with a positional encoder to attend not just at the tokens present in a phrase but also their arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "697d4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPTMoE(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layers=4, dim=128, n_heads=4,\n",
    "                 mlp_hidden_dim=512, n_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.dim = dim\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, dim)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlockMoE(dim, n_heads, mlp_hidden_dim, n_experts=n_experts, top_k=top_k)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, T)\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        tok = self.token_emb(idx)  # (B, T, C)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))[None, :, :]\n",
    "\n",
    "        x = self.drop(tok + pos)\n",
    "\n",
    "        gate_probs_all = []\n",
    "        for blk in self.blocks:\n",
    "            x, gate_probs = blk(x)\n",
    "            gate_probs_all.append(gate_probs)  # list of (B, T, n_experts)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab)\n",
    "        # stack gate probs along layers -> (n_layers, B, T, n_experts)\n",
    "        gate_probs_all = torch.stack(gate_probs_all, dim=0)\n",
    "        return logits, gate_probs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd6fcd",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To simplify the training process, I provide a training helper using an `AdamW` optimizer, a schedular with decaying learning rate and defining an evaluation function comparing the cross-entropy loss between the predicted and correct answers on the validation dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9fa59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(dataloader, desc=\"Evaluando\", leave=False):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits, _ = model(xb)\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), yb.view(B*T), reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "            n_tokens += B*T\n",
    "    model.train()\n",
    "    return total_loss / n_tokens\n",
    "\n",
    "def train(tokenizer, train_loader, val_loader, device, block_size=128, n_layers=4, dim=128, n_heads=4, mlp_hidden_dim=512,\n",
    "          n_experts=4, top_k=1, lr=3e-4, epochs=20, log_interval=100, ckpt_path=\"tinygpt_moe.pth\"):\n",
    "\n",
    "    vocab_size = len(tokenizer.get_vocab())\n",
    "    model = TinyGPTMoE(vocab_size=vocab_size, block_size=block_size,\n",
    "                       n_layers=n_layers, dim=dim, n_heads=n_heads,\n",
    "                       mlp_hidden_dim=mlp_hidden_dim, n_experts=n_experts, top_k=top_k)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-1)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_tokens = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "        for i, (xb, yb) in enumerate(progress_bar, start=1):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits, gate_probs = model(xb)\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), yb.view(B*T))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * B*T\n",
    "            n_tokens += B*T\n",
    "\n",
    "            if (i+1) % log_interval == 0:\n",
    "                avg_loss = running_loss / n_tokens\n",
    "                print(f\"Epoch {epoch} iter {i+1}/{len(train_loader)} loss {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        val_loss = evaluate(model, val_loader, device)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        print(f\"==> Epoch {epoch} validation loss {val_loss:.4f} ppl {val_ppl:.2f}\")\n",
    "\n",
    "        # save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state': model.state_dict(),\n",
    "                'tokenizer': tokenizer.__dict__\n",
    "            }, ckpt_path)\n",
    "            print(\"Modelo guardado en\", ckpt_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe9e4b",
   "metadata": {},
   "source": [
    "We set the needed parameters and start the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb713d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  10%|▉         | 99/993 [06:30<58:23,  3.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 100/993 loss 6.7557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  20%|██        | 199/993 [13:03<51:36,  3.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 200/993 loss 6.2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  30%|███       | 299/993 [19:37<44:46,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 300/993 loss 5.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  40%|████      | 399/993 [27:04<50:15,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 400/993 loss 5.6822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  50%|█████     | 499/993 [35:19<39:41,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 500/993 loss 5.3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  60%|██████    | 599/993 [44:37<36:16,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 600/993 loss 4.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  70%|███████   | 699/993 [55:16<31:36,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 700/993 loss 4.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  80%|████████  | 799/993 [1:05:56<20:56,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 800/993 loss 4.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2:  91%|█████████ | 899/993 [1:15:40<07:37,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 900/993 loss 3.6563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/2: 100%|██████████| 993/993 [1:23:04<00:00,  5.02s/it]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 0 validation loss 0.8322 ppl 2.30\n",
      "Modelo guardado en tinygpt_moe.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  10%|▉         | 99/993 [08:11<1:13:37,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 100/993 loss 0.5234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  20%|██        | 199/993 [16:31<1:05:49,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 200/993 loss 0.4642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  30%|███       | 299/993 [24:41<58:39,  5.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 300/993 loss 0.4155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  40%|████      | 399/993 [32:42<47:01,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 400/993 loss 0.3753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  50%|█████     | 499/993 [40:49<42:36,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 500/993 loss 0.3421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  60%|██████    | 599/993 [49:04<32:11,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 600/993 loss 0.3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  70%|███████   | 699/993 [57:16<23:48,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 700/993 loss 0.2910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  80%|████████  | 799/993 [1:05:41<16:44,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 800/993 loss 0.2711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  91%|█████████ | 899/993 [1:14:10<07:50,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iter 900/993 loss 0.2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 993/993 [1:21:55<00:00,  4.95s/it]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 validation loss 0.4799 ppl 1.62\n",
      "Modelo guardado en tinygpt_moe.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "block_size = 128\n",
    "n_layers = 4\n",
    "dim = 128\n",
    "n_heads = 4\n",
    "mlp_hidden_dim = 512\n",
    "n_experts = 4\n",
    "top_k = 1\n",
    "lr = 3e-4\n",
    "epochs = 2\n",
    "log_interval = 100\n",
    "ckpt_path = \"tinygpt_moe.pth\"\n",
    "\n",
    "# Start training\n",
    "model = train(tokenizer, train_loader, val_loader, DEVICE,\n",
    "      block_size=block_size,\n",
    "      n_layers=n_layers,\n",
    "      dim=dim,\n",
    "      n_heads=n_heads,\n",
    "      mlp_hidden_dim=mlp_hidden_dim,\n",
    "      n_experts=n_experts,\n",
    "      top_k=top_k,\n",
    "      lr=lr,\n",
    "      epochs=epochs,\n",
    "      log_interval=log_interval,\n",
    "      ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d143c",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "After training the model we are able to produce inferences based on a certain seed. The model will provide a list of tokens oredered by descending probability to be chosen and we define three different strategies to ensamble the tokens to produce a coherent response. The algorithms chosen for this operation are:\n",
    "\n",
    "* Greedy approach: This is the most straightforward algorithm and consist on choosing the token with the highest probability.\n",
    "\n",
    "* Top-k approach: We select the *k* most probable tokens and then choose one of them at random, weighted by their probabilities.\n",
    "\n",
    "* Top-p approach: We select groups of tokens called *nucleus* where tokens are included until the cummulative probability goes over a threshold *p*. Then we choose one of them at random, weighted by their probabilities.\n",
    "\n",
    "Additionally, we implement a *Temperature Sampling* mechanism that narrows or expands the ability of the infered phrase to allow a wide variety of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b28956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_greedy(model, tokenizer, seed_text, max_new_tokens=200, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    idx = torch.tensor([tokenizer.encode(seed_text).ids], dtype=torch.long, device=device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        if idx.shape[1] > model.block_size:\n",
    "            idx = idx[:, -model.block_size:]\n",
    "        logits, _ = model(idx)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "    out = model.token_emb.weight.device\n",
    "    return tokenizer.decode(idx[0].tolist())\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_p_filtering(logits, top_p=0.9, filter_value=-float(\"Inf\")):\n",
    "    \"\"\"Filtra logits según probabilidad acumulada top-p (nucleus).\"\"\"\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # Determina los índices a filtrar\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Siempre dejamos al menos el primer token\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "    # Reconstruimos el tensor original\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "    return logits\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sample(model, tokenizer, seed_text, max_new_tokens, device,\n",
    "                    temperature=1.0, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # tokenizar entrada\n",
    "    encoded = tokenizer.encode(seed_text).ids\n",
    "    idx = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # recortar si se pasa del tamaño de contexto\n",
    "        if idx.shape[1] > model.block_size:\n",
    "            idx_cond = idx[:, -model.block_size:]\n",
    "        else:\n",
    "            idx_cond = idx\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        # --- top-k filtering ---\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            min_values = values[:, -1].unsqueeze(1)\n",
    "            logits = torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "        # --- top-p (nucleus) filtering ---\n",
    "        if top_p is not None and 0 < top_p < 1.0:\n",
    "            logits = top_p_filtering(logits, top_p)\n",
    "\n",
    "        # --- softmax y muestreo ---\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # normalizamos por si hay NaN o todos inf\n",
    "        probs = torch.nan_to_num(probs, nan=0.0)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        if torch.all(probs == 0):\n",
    "            # fallback a greedy si las probabilidades son inválidas\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4be5b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejemplos de generación ---\n",
      "Greedy:\n",
      " ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Sample (temp=1.0, top_k=40):\n",
      " To be, or,,,,,,,,,,,,,,,,,,,,,,,,,, worthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, Warwick,,,,, and him,,,,,,,,,,,,, he,,, with, if,,,,,,,, let, if, I, more friends,,,,,, of,,,,,,,, it,,,,,, your,,,,, thou, all,,\n",
      "\n",
      "Sample (temp=0.8, top_p=0.9):\n",
      " To be, or hide utter bless ban think think be think be think be think be think be think be think be shines hidegar step obey obey inform obey meet obey think obey think be obey obey hideDeathuresLies be obey be obey think be obey be obey obey think push bless tell pursuit obey.' hide obey couldst extrem obey obey be think be think be mark notes BOLINGBROKE be obey think obey think obey think obey bless think obeyats be deserved obey enough sha tear think obey wash obey obeyakerceiving think be be obey begg think obeyiest obey obeyaker pow obey think obey think obeyfather hold storms stumble follow think obey suits obey fear throne obeyar pursuit obey thy gifts tell be man by deathbe retiregelack esteem Marcius Henry noteutus up be kill death follow:--, put retire crave needs be tear think Paulina doth be put be, obeybe obey thy right obey keepon to thyself Edward be beheld slay: throw tide boast un attended stale to beg be may be be blest first crave finds be be boundf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# final sample\n",
    "print(\"\\n--- Ejemplos de generación ---\")\n",
    "seed = \"To be, or\"\n",
    "print(\"Greedy:\\n\", generate_greedy(model, tokenizer, seed, max_new_tokens=200, device=DEVICE))\n",
    "print(\"\\nSample (temp=1.0, top_k=40):\\n\", generate_sample(model, tokenizer, seed, max_new_tokens=200,\n",
    "                                                        device=DEVICE, temperature=1.2, top_k=40))\n",
    "print(\"\\nSample (temp=0.8, top_p=0.9):\\n\", generate_sample(model, tokenizer, seed, max_new_tokens=200,\n",
    "                                                            device=DEVICE, temperature=1.2, top_p=0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TinyGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
